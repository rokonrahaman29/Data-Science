{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rokonrahamanshaikh/dl-exp3?scriptVersionId=218404370\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"cb93f0df","metadata":{"execution":{"iopub.execute_input":"2025-01-20T07:05:27.49052Z","iopub.status.busy":"2025-01-20T07:05:27.490116Z","iopub.status.idle":"2025-01-20T07:05:30.728612Z","shell.execute_reply":"2025-01-20T07:05:30.72731Z"},"papermill":{"duration":3.244035,"end_time":"2025-01-20T07:05:30.73051","exception":false,"start_time":"2025-01-20T07:05:27.486475","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 2992.6133\n","Epoch 10, Loss: 2998.4990\n","Epoch 20, Loss: 2950.3848\n","Epoch 30, Loss: 3173.7616\n","Epoch 40, Loss: 3048.0829\n","Epoch 50, Loss: 3122.1761\n","Epoch 60, Loss: 3115.8084\n","Epoch 70, Loss: 2992.2659\n","Epoch 80, Loss: 2998.4720\n","Epoch 90, Loss: 3089.7516\n","Mean Squared Error on Test Set: 3299.1816\n"]}],"source":["import numpy as np\n","from sklearn.datasets import load_diabetes\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load and preprocess the diabetes dataset\n","data = load_diabetes()\n","X, y = data.data, data.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Stochastic Gradient Descent function\n","def stochastic_gradient_descent(X, y, learning_rate=0.01, num_epochs=100, batch_size=1):\n","    num_samples, num_features = X.shape\n","    weights = np.zeros(num_features)\n","    bias = 0\n","    \n","    # Training loop\n","    for epoch in range(num_epochs):\n","        for _ in range(0, num_samples, batch_size):\n","            random_indices = np.random.choice(num_samples, batch_size, replace=False)\n","            X_batch = X[random_indices]\n","            y_batch = y[random_indices]\n","            \n","            predictions = np.dot(X_batch, weights) + bias\n","            errors = predictions - y_batch\n","            \n","            gradient_weights = np.dot(X_batch.T, errors) / batch_size\n","            gradient_bias = np.sum(errors) / batch_size\n","            \n","            # Update parameters\n","            weights -= learning_rate * gradient_weights\n","            bias -= learning_rate * gradient_bias\n","        \n","        # Print loss every 10 epochs\n","        if epoch % 10 == 0:\n","            loss = np.mean((np.dot(X, weights) + bias - y) ** 2)\n","            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","    \n","    return weights, bias\n","\n","# Train the model using stochastic gradient descent\n","weights, bias = stochastic_gradient_descent(X_train_scaled, y_train, learning_rate=0.01, num_epochs=100)\n","\n","# Evaluate the model on the test set\n","predictions = np.dot(X_test_scaled, weights) + bias\n","mse = np.mean((predictions - y_test) ** 2)\n","print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"5da5917e","metadata":{"papermill":{"duration":0.002039,"end_time":"2025-01-20T07:05:30.735407","exception":false,"start_time":"2025-01-20T07:05:30.733368","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"id":"81181eda","metadata":{"execution":{"iopub.execute_input":"2025-01-20T07:05:30.741463Z","iopub.status.busy":"2025-01-20T07:05:30.740912Z","iopub.status.idle":"2025-01-20T07:05:30.969475Z","shell.execute_reply":"2025-01-20T07:05:30.968229Z"},"papermill":{"duration":0.233827,"end_time":"2025-01-20T07:05:30.971499","exception":false,"start_time":"2025-01-20T07:05:30.737672","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 13364.5380\n","Epoch 10, Loss: 2918.2617\n","Epoch 20, Loss: 2955.3184\n","Epoch 30, Loss: 2911.9763\n","Epoch 40, Loss: 2903.8295\n","Epoch 50, Loss: 2921.0020\n","Epoch 60, Loss: 2896.2017\n","Epoch 70, Loss: 2892.1956\n","Epoch 80, Loss: 2893.2978\n","Epoch 90, Loss: 2906.4344\n","Mean Squared Error on Test Set: 2893.2056\n"]}],"source":["import numpy as np\n","from sklearn.datasets import load_diabetes\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load and preprocess the diabetes dataset\n","data = load_diabetes()\n","X, y = data.data, data.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Stochastic Gradient Descent function\n","def stochastic_gradient_descent(X, y, learning_rate=0.01, num_epochs=100, batch_size=2**3):\n","    num_samples, num_features = X.shape\n","    weights = np.zeros(num_features)\n","    bias = 0\n","    \n","    # Training loop\n","    for epoch in range(num_epochs):\n","        for _ in range(0, num_samples, batch_size):\n","            random_indices = np.random.choice(num_samples, batch_size, replace=False)\n","            X_batch = X[random_indices]\n","            y_batch = y[random_indices]\n","            \n","            predictions = np.dot(X_batch, weights) + bias\n","            errors = predictions - y_batch\n","            \n","            gradient_weights = np.dot(X_batch.T, errors) / batch_size\n","            gradient_bias = np.sum(errors) / batch_size\n","            \n","            # Update parameters\n","            weights -= learning_rate * gradient_weights\n","            bias -= learning_rate * gradient_bias\n","        \n","        # Print loss every 10 epochs\n","        if epoch % 10 == 0:\n","            loss = np.mean((np.dot(X, weights) + bias - y) ** 2)\n","            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n","    \n","    return weights, bias\n","\n","# Train the model using stochastic gradient descent\n","weights, bias = stochastic_gradient_descent(X_train_scaled, y_train, learning_rate=0.01, num_epochs=100)\n","\n","# Evaluate the model on the test set\n","predictions = np.dot(X_test_scaled, weights) + bias\n","mse = np.mean((predictions - y_test) ** 2)\n","print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"7336b7eb","metadata":{"papermill":{"duration":0.002261,"end_time":"2025-01-20T07:05:30.976519","exception":false,"start_time":"2025-01-20T07:05:30.974258","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2527538,"sourceId":4289678,"sourceType":"datasetVersion"}],"dockerImageVersionId":30839,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":6.989921,"end_time":"2025-01-20T07:05:31.600724","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-20T07:05:24.610803","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}